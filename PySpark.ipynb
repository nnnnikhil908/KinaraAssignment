{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3caf64de-1b6d-4431-8abe-16a65aed1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, date_format, trim, concat, col, lit, datediff, when, regexp_replace, regexp_extract\n",
    "import psycopg2\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb189a7-13b7-43df-8718-90c36bfbf8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/02 13:09:40 WARN Utils: Your hostname, Nikhils-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.101 instead (on interface en0)\n",
      "24/11/02 13:09:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/11/02 13:09:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.Builder().appName(\"ETL_Assignment\").master(\"local[*]\").config(\"spark.jars\", \"postgresql-42.7.4.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e39c3d3-640d-47ed-96ba-f768b29a9bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filePath):\n",
    "    \"\"\"\n",
    "    This function will read data from the arg filePath (which should be a string) and will return the data as a PySpark data frame.\n",
    "    Since I don't have expertise working with Padas, I have used PySpark.\n",
    "    I can do the same using Pandas, but I will need the help of the GenAI tool, and at this moment, I don't want to do this.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        employee_df = spark.read.option(\"header\", True).csv(filePath)\n",
    "        return employee_df\n",
    "    except:\n",
    "        print(\"Kuch to gadbad hai Daya, Data read nahi ho raha hai\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1100a35-08b8-4aab-aac9-f8ca9d226d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------+----------+-----------+-------+\n",
      "|EmployeeID|FirstName|    LastName  | BirthDate| Department| Salary|\n",
      "+----------+---------+--------------+----------+-----------+-------+\n",
      "|  E001    |  Alice  |     White    |1990-06-12|  Finance  |  55000|\n",
      "|  E002    |    Bob  |     Brown    |1988-01-03| IT        |  90000|\n",
      "|  E003    |   Carol |     Grey     |1995-07-15| HR        |  47000|\n",
      "|  E004    |   David |     Black    |1992-09-23| Marketing |  75000|\n",
      "|  E005    |   Eve   |     Green    |1985-03-18| IT        | 125000|\n",
      "|  E006    |   Frank |     Turner   |1980-11-21| Finance   |  85000|\n",
      "|  E007    |   Grace |     Harper   |1993-02-15| HR        |  62000|\n",
      "|  E008    |    Holly|     Norman   |1999-04-09| Marketing |  43000|\n",
      "|  E009    |     Ian |     Ross     |1996-05-27| Finance   | 110000|\n",
      "|  E010    |   John  |     Bishop   |1997-08-18| IT        |  70000|\n",
      "|EmployeeID|FirstName|    LastName  | BirthDate| Department| Salary|\n",
      "|  E011    |    Ada  |    L Lovelace|1803-05-23| IT        | 100000|\n",
      "|  E012    |  Amy C  |   Appleseed  |1976-02-29| IT        | -87500|\n",
      "|  E013    |   Bao   |    Nguyen    |2005-12-31| Marketing | 200150|\n",
      "|  E014    |    Carla|   342 Jimenez|1975-03-03| Finance   | -39400|\n",
      "|  E015    | 007 Dena|  Smith-Tracy |1980-01-01| HR        | 10000A|\n",
      "|  E016    | Ellie J.|  McException |2023-11-20| IT        |  75000|\n",
      "|  E017    |   (Inna)|       Romanov|1988-02-01| Finance   | 102000|\n",
      "|  E018    |    Liska|        -diana|1991-12-20| HR        |  49999|\n",
      "|  E019    |  Mario  |    Antonio   |1985-05-05| Marketing | 100000|\n",
      "+----------+---------+--------------+----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath = \"employee_details.csv\"\n",
    "read = read_csv(filePath)\n",
    "read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18992a45-999f-45b4-af0e-72e66c65cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"\"\"\n",
    "    This function transforms the input DataFrame by performing various cleaning and transformation operations as per requirement.\n",
    "\n",
    "    This function will require a DF as an argument and will return a df.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert BirthDate to Date type and filter out nulls\n",
    "    df = (df.withColumn(\"BirthDate\", to_date(col(\"BirthDate\"), \"yyyy-MM-dd\"))\n",
    "            .filter(col(\"BirthDate\").isNotNull()))\n",
    "\n",
    "\n",
    "    # Format BirthDate to DD/MM/YYYY\n",
    "    df = df.withColumn(\"birthdate\", date_format(col(\"BirthDate\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "    # Extract digits from Salary (remove \"-\") and convert to Integer for further transformations\n",
    "    df = (df.withColumn(\"Salary\", regexp_extract(col(\"Salary\"), \"\\\\d+\", 0))\n",
    "            .filter(col(\"Salary\") != \"\")\n",
    "            .withColumn(\"Salary\", col(\"Salary\").cast(\"int\")))\n",
    "\n",
    "\n",
    "    # Trim spaces and remove special characters from FirstName and LastName\n",
    "    column = df.columns\n",
    "    df = (df.withColumn(\"FirstName\", trim(regexp_replace(column[1], \"[^A-Za-z\\\\s]\", \"\")))\n",
    "            .withColumn(\"LastName\", trim(regexp_replace(column[2], \"[^A-Za-z\\\\s]\", \"\"))))\n",
    "\n",
    "    # FullName column by concatenating FirstName and LastName\n",
    "    df = df.withColumn(\"fullName\", concat(col(\"FirstName\"), lit(\" \"), col(\"LastName\")))\n",
    "\n",
    "    # Age with reference to 01-01-2023\n",
    "    df = df.withColumn(\"Age\", \n",
    "                       (datediff(to_date(lit(\"2023-01-01\"), \"yyyy-MM-dd\"), \n",
    "                                 to_date(col(\"BirthDate\"), \"dd/MM/yyyy\")) / 365).cast(\"int\"))\n",
    "\n",
    "    #SalaryBucket based on Salary\n",
    "    df = (df.withColumn(\"SalaryBucket\", \n",
    "                        when(col(\"Salary\") < 50000, \"A\")\n",
    "                        .when((col(\"Salary\") >= 50000) & (col(\"Salary\") < 100000), \"B\")\n",
    "                        .otherwise(\"C\")))\n",
    "    df = df.drop(column[1], column[2],\"LastName\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddf0c101-70da-4983-b79f-5de62b4a74a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+------+-------------------+---+------------+\n",
      "|EmployeeID| birthdate| Department|Salary|           fullName|Age|SalaryBucket|\n",
      "+----------+----------+-----------+------+-------------------+---+------------+\n",
      "|  E001    |12/06/1990|  Finance  | 55000|        Alice White| 32|           B|\n",
      "|  E002    |03/01/1988| IT        | 90000|          Bob Brown| 35|           B|\n",
      "|  E003    |15/07/1995| HR        | 47000|         Carol Grey| 27|           A|\n",
      "|  E004    |23/09/1992| Marketing | 75000|        David Black| 30|           B|\n",
      "|  E005    |18/03/1985| IT        |125000|          Eve Green| 37|           C|\n",
      "|  E006    |21/11/1980| Finance   | 85000|       Frank Turner| 42|           B|\n",
      "|  E007    |15/02/1993| HR        | 62000|       Grace Harper| 29|           B|\n",
      "|  E008    |09/04/1999| Marketing | 43000|       Holly Norman| 23|           A|\n",
      "|  E009    |27/05/1996| Finance   |110000|           Ian Ross| 26|           C|\n",
      "|  E010    |18/08/1997| IT        | 70000|        John Bishop| 25|           B|\n",
      "|  E011    |23/05/1803| IT        |100000|     Ada L Lovelace|219|           C|\n",
      "|  E012    |29/02/1976| IT        | 87500|    Amy C Appleseed| 46|           B|\n",
      "|  E013    |31/12/2005| Marketing |200150|         Bao Nguyen| 17|           C|\n",
      "|  E014    |03/03/1975| Finance   | 39400|      Carla Jimenez| 47|           A|\n",
      "|  E015    |01/01/1980| HR        | 10000|    Dena SmithTracy| 43|           A|\n",
      "|  E016    |20/11/2023| IT        | 75000|Ellie J McException|  0|           B|\n",
      "|  E017    |01/02/1988| Finance   |102000|       Inna Romanov| 34|           C|\n",
      "|  E018    |20/12/1991| HR        | 49999|        Liska diana| 31|           A|\n",
      "|  E019    |05/05/1985| Marketing |100000|      Mario Antonio| 37|           C|\n",
      "|  E021    |01/06/2018| IT        | 30000|    Angela Williams|  4|           A|\n",
      "+----------+----------+-----------+------+-------------------+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df = transform_data(read)\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b6e1893-620c-424c-bf82-5f41ba0271ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, table_name: str):\n",
    "    \"\"\"\n",
    "    Loads transformed DataFrame into a PostgreSQL database table and creates indexes.\n",
    "    Note: The url for Postgre is hardcoded. I have created a test Postgre server for this. \n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to load into the database.\n",
    "    table_name (str): The name of the target table in the database.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the table name to lowercase to avoid case issues\n",
    "    table_name = table_name.lower()\n",
    "\n",
    "    # PostgreSQL connection details\n",
    "    db_url = 'jdbc:postgresql://pg-nikhiltest-nnn-nikhiltest.h.aivencloud.com:17081/defaultdb?sslmode=require'\n",
    "    \n",
    "    # Extract connection parameters for psycopg2 connection\n",
    "    url_parts = urllib.parse.urlparse('postgres://avnadmin:AVNS_yGpIBK9-f-QpZ5bA7ri@pg-nikhiltest-nnn-nikhiltest.h.aivencloud.com:17081/defaultdb?sslmode=require')\n",
    "    dbname = url_parts.path[1:]  # Get database name from db_url (removing the leading '/')\n",
    "    host = url_parts.hostname\n",
    "    port = url_parts.port\n",
    "    user = url_parts.username\n",
    "    password = url_parts.password\n",
    "\n",
    "    # Create a connection to PostgreSQL\n",
    "    try:\n",
    "        conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host, port=port)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to the database: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Drop the table if it exists\n",
    "        drop_table_query = f\"DROP TABLE IF EXISTS {table_name};\"\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(drop_table_query)\n",
    "            conn.commit()\n",
    "            print(f\"Dropped table: {table_name}\")\n",
    "\n",
    "        # Create the table with lowercase naming\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            employeeid SERIAL PRIMARY KEY,\n",
    "            birthdate DATE NOT NULL,\n",
    "            department VARCHAR(100) NOT NULL,\n",
    "            salary NUMERIC(10, 2) NOT NULL,\n",
    "            fullname VARCHAR(200) NOT NULL,\n",
    "            age INT NOT NULL,\n",
    "            salarybucket VARCHAR(50) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(create_table_query)\n",
    "            conn.commit()\n",
    "            print(f\"Created table: {table_name}\")\n",
    "\n",
    "        # Check if table was created\n",
    "        check_table_query = f\"\"\"\n",
    "        SELECT EXISTS (\n",
    "            SELECT FROM information_schema.tables \n",
    "            WHERE table_name = '{table_name}'\n",
    "        );\n",
    "        \"\"\"\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(check_table_query)\n",
    "            table_exists = cursor.fetchone()[0]\n",
    "            if table_exists:\n",
    "                print(f\"Table '{table_name}' was successfully created.\")\n",
    "            else:\n",
    "                print(f\"Table '{table_name}' does not exist after creation attempt.\")\n",
    "\n",
    "        # Convert DataFrame column names to lowercase\n",
    "        df = df.toDF(*[col.lower() for col in df.columns])\n",
    "\n",
    "        # Print DataFrame schema for debugging\n",
    "        print(\"DataFrame schema:\")\n",
    "        df.printSchema()\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_columns = ['birthdate', 'department', 'salary', 'fullname', 'age', 'salarybucket']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "            return\n",
    "\n",
    "        # Write the DataFrame directly to PostgreSQL using Spark's DataFrameWriter\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", db_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", user) \\\n",
    "            .option(\"password\", password) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "\n",
    "        # Create indexes for enhancing retrieval performance\n",
    "        index_queries = [\n",
    "            f\"CREATE INDEX IF NOT EXISTS idx_fullname ON {table_name} (fullname);\",\n",
    "            f\"CREATE INDEX IF NOT EXISTS idx_age ON {table_name} (age);\",\n",
    "            f\"CREATE INDEX IF NOT EXISTS idx_salary ON {table_name} (salary);\",\n",
    "            f\"CREATE INDEX IF NOT EXISTS idx_salarybucket ON {table_name} (salarybucket);\"\n",
    "        ]\n",
    "        \n",
    "        # Execute each index creation query\n",
    "        for query in index_queries:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(query)\n",
    "                conn.commit()\n",
    "                print(f\"Created index with query: {query}\")\n",
    "\n",
    "        print(f\"Data successfully loaded into {table_name} and indexes created.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Ensure the connection is closed\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cd0e02f-b0b3-4ccb-a5ef-033dd4288b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped table if it existed: employees\n",
      "Created table: employees\n",
      "Table 'employees' was successfully created.\n",
      "DataFrame schema:\n",
      "root\n",
      " |-- employeeid: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- fullname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salarybucket: string (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index with query: CREATE INDEX IF NOT EXISTS idx_fullname ON employees (fullname);\n",
      "Created index with query: CREATE INDEX IF NOT EXISTS idx_age ON employees (age);\n",
      "Created index with query: CREATE INDEX IF NOT EXISTS idx_salary ON employees (salary);\n",
      "Created index with query: CREATE INDEX IF NOT EXISTS idx_salarybucket ON employees (salarybucket);\n",
      "Data successfully loaded into employees and indexes created.\n"
     ]
    }
   ],
   "source": [
    "#Execution code\n",
    "\n",
    "filePath = \"employee_details.csv\"\n",
    "read = read_csv(filePath)\n",
    "#read.show()\n",
    "\n",
    "\n",
    "transformed_df = transform_data(read)\n",
    "\n",
    "\n",
    "load_data(transformed_df, \"employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439614c2-d182-4f3b-b276-3be21ae63344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
